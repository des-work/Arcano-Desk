import React, { createContext, useContext, useState, useCallback } from 'react';
import { analyzeContent, getModelInfo, AVAILABLE_MODELS } from '../utils/contentAnalyzer.ts';
import { generateTopicPrompt, suggestTopicsFromContent, createTopicFromQuestion, formatGeneratedContent } from '../utils/topicGenerator.ts';

interface OllamaContextType {
  isConnected: boolean;
  models: string[];
  currentModel: string;
  isLoading: boolean;
  connect: () => Promise<boolean>;
  generateSummary: (content: string, length: 'short' | 'medium' | 'long', format: string) => Promise<string>;
  generateStudyMaterial: (content: string, type: 'flashcards' | 'questions' | 'notes') => Promise<string>;
  askQuestion: (question: string, context: string) => Promise<string>;
  generateTopic: (topic: string, type: 'definition' | 'example' | 'explanation' | 'study_guide', depth: 'brief' | 'detailed' | 'comprehensive', context?: string) => Promise<string>;
  suggestTopics: (content: string) => Promise<any[]>;
  setCurrentModel: (model: string) => void;
  getModelInfo: (modelName: string) => any;
  analyzeContent: (content: string) => any;
}

const OllamaContext = createContext<OllamaContextType | undefined>(undefined);

export const useOllama = () => {
  const context = useContext(OllamaContext);
  if (!context) {
    throw new Error('useOllama must be used within an OllamaProvider');
  }
  return context;
};

export const OllamaProvider: React.FC<{ children: React.ReactNode }> = ({ children }) => {
  const [isConnected, setIsConnected] = useState(false);
  const [models, setModels] = useState<string[]>([]);
  const [currentModel, setCurrentModel] = useState('llama2');
  const [isLoading, setIsLoading] = useState(false);

  const connect = useCallback(async (): Promise<boolean> => {
    try {
      setIsLoading(true);
      const response = await fetch('http://localhost:11434/api/tags');
      const data = await response.json();
      const availableModels = data.models?.map((model: any) => model.name) || [];
      setModels(availableModels);
      setIsConnected(true);
      return true;
    } catch (error) {
      console.error('Failed to connect to Ollama:', error);
      setIsConnected(false);
      return false;
    } finally {
      setIsLoading(false);
    }
  }, []);

  const generateSummary = useCallback(async (
    content: string, 
    length: 'short' | 'medium' | 'long', 
    format: string
  ): Promise<string> => {
    try {
      setIsLoading(true);
      
      // Analyze content to determine best model
      const analysis = analyzeContent(content);
      const recommendedModel = analysis.recommendedModel;
      const modelToUse = models.includes(recommendedModel) ? recommendedModel : currentModel;
      
      // Show wizard's reasoning
      const modelInfo = getModelInfo(modelToUse);
      console.log(`üßô‚Äç‚ôÇÔ∏è Wizard's Analysis: ${analysis.reasoning}`);
      console.log(`üìä Content Stats: ${analysis.wordCount} words, complexity: ${analysis.complexityScore}/10`);
      console.log(`ü§ñ Using Model: ${modelToUse} (${modelInfo?.size || 'unknown size'})`);

      const lengthInstructions = {
        short: 'Create a brief 1-paragraph summary',
        medium: 'Create a detailed 3-paragraph summary',
        long: 'Create a comprehensive multi-paragraph summary'
      };

      const prompt = `You are a magical study assistant wizard. ${lengthInstructions[length]} of the following content in ${format} format. 

Focus on key terms, dates, formulas, and important concepts. Make it engaging, well-organized, and easy to understand. Avoid messy regurgitations - provide clear, structured insights.

Content to analyze:
${content}

Magical Summary:`;

      const response = await fetch('http://localhost:11434/api/generate', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: modelToUse,
          prompt,
          stream: false,
          options: {
            temperature: 0.7,
            top_p: 0.9,
          }
        })
      });

      const data = await response.json();
      const summary = data.response || 'Failed to generate summary';
      
      // Add model transparency
      return `${summary}\n\n---\n*Generated by ${modelToUse} (${modelInfo?.size || 'unknown size'}) - ${analysis.reasoning}*`;
    } catch (error) {
      console.error('Failed to generate summary:', error);
      return 'Error generating summary. Please check your Ollama connection.';
    } finally {
      setIsLoading(false);
    }
  }, [currentModel, models]);

  const generateStudyMaterial = useCallback(async (
    content: string, 
    type: 'flashcards' | 'questions' | 'notes'
  ): Promise<string> => {
    try {
      setIsLoading(true);
      
      const typeInstructions = {
        flashcards: 'Create flashcards with questions and answers',
        questions: 'Create practice questions with detailed answers',
        notes: 'Create organized study notes with key concepts'
      };

      const prompt = `You are a magical study assistant. ${typeInstructions[type]} based on the following content. Make it engaging and educational:

${content}

${type.charAt(0).toUpperCase() + type.slice(1)}:`;

      const response = await fetch('http://localhost:11434/api/generate', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: currentModel,
          prompt,
          stream: false,
          options: {
            temperature: 0.7,
            top_p: 0.9,
          }
        })
      });

      const data = await response.json();
      return data.response || 'Failed to generate study material';
    } catch (error) {
      console.error('Failed to generate study material:', error);
      return 'Error generating study material. Please check your Ollama connection.';
    } finally {
      setIsLoading(false);
    }
  }, [currentModel]);

  const askQuestion = useCallback(async (
    question: string, 
    context: string
  ): Promise<string> => {
    try {
      setIsLoading(true);
      
      // Analyze context to determine best model
      const analysis = analyzeContent(context);
      const recommendedModel = analysis.recommendedModel;
      const modelToUse = models.includes(recommendedModel) ? recommendedModel : currentModel;
      
      // Show wizard's reasoning
      const modelInfo = getModelInfo(modelToUse);
      console.log(`üßô‚Äç‚ôÇÔ∏è Wizard's Analysis: ${analysis.reasoning}`);
      console.log(`ü§ñ Using Model: ${modelToUse} (${modelInfo?.size || 'unknown size'})`);

      const prompt = `You are a magical study assistant wizard. Answer the following question based on the provided context. Be confident, helpful, and well-organized. Provide clear, structured responses that avoid messy regurgitations.

Context: ${context}

Question: ${question}

Magical Answer:`;

      const response = await fetch('http://localhost:11434/api/generate', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: modelToUse,
          prompt,
          stream: false,
          options: {
            temperature: 0.7,
            top_p: 0.9,
          }
        })
      });

      const data = await response.json();
      const answer = data.response || 'Failed to generate answer';
      
      // Add model transparency
      return `${answer}\n\n---\n*Answered by ${modelToUse} (${modelInfo?.size || 'unknown size'}) - ${analysis.reasoning}*`;
    } catch (error) {
      console.error('Failed to generate answer:', error);
      return 'Error generating answer. Please check your Ollama connection.';
    } finally {
      setIsLoading(false);
    }
  }, [currentModel, models]);

  const generateTopic = useCallback(async (
    topic: string,
    type: 'definition' | 'example' | 'explanation' | 'study_guide',
    depth: 'brief' | 'detailed' | 'comprehensive',
    context?: string
  ): Promise<string> => {
    try {
      setIsLoading(true);
      
      // For topic generation, use a balanced model
      const modelToUse = models.includes('phi3:latest') ? 'phi3:latest' : 
                        models.includes('llama2:latest') ? 'llama2:latest' : 
                        currentModel;
      
      const modelInfo = getModelInfo(modelToUse);
      console.log(`üßô‚Äç‚ôÇÔ∏è Generating ${type} for "${topic}" using ${modelToUse}`);

      const prompt = generateTopicPrompt({ topic, type, depth, context });
      
      const response = await fetch('http://localhost:11434/api/generate', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: modelToUse,
          prompt,
          stream: false,
          options: {
            temperature: 0.7,
            top_p: 0.9,
          }
        })
      });

      const data = await response.json();
      const content = data.response || 'Failed to generate topic';
      const formattedContent = formatGeneratedContent(content, type);
      
      return `${formattedContent}\n\n---\n*Generated by ${modelToUse} (${modelInfo?.size || 'unknown size'})*`;
    } catch (error) {
      console.error('Failed to generate topic:', error);
      return 'Error generating topic. Please check your Ollama connection.';
    } finally {
      setIsLoading(false);
    }
  }, [currentModel, models]);

  const suggestTopics = useCallback(async (content: string): Promise<any[]> => {
    try {
      const analysis = analyzeContent(content);
      const suggestions = suggestTopicsFromContent(content, analysis);
      return suggestions;
    } catch (error) {
      console.error('Failed to suggest topics:', error);
      return [];
    }
  }, []);

  const value: OllamaContextType = {
    isConnected,
    models,
    currentModel,
    isLoading,
    connect,
    generateSummary,
    generateStudyMaterial,
    askQuestion,
    generateTopic,
    suggestTopics,
    setCurrentModel,
    getModelInfo: (modelName: string) => getModelInfo(modelName),
    analyzeContent: (content: string) => analyzeContent(content),
  };

  return (
    <OllamaContext.Provider value={value}>
      {children}
    </OllamaContext.Provider>
  );
};
